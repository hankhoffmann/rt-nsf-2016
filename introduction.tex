In recent years, the microprocessor industry has turned to heterogeneous multi-core processor designs for the next generation of embedded systems. By integrating relatively slower, lower-power processor cores with relatively more powerful and power-hungry ones, it is possible to dramatically improve energy efficiency while achieving high performance. %, particularly for the emerging dynamic computing needs seen in many application domains.  
 %the number of cores, it is possible to dramatically improve performance as well as energy efficiency. This trend has brought in a new breed of processors---\textit{many-core with network-on-chip}---which provide unprecedented advantage in terms of performance-per-watt and inter-core communication latency over traditional microprocessor architectures. 
%Per-chip core counts are expected to further increase significantly in the near future. Indeed, Intel and Tilera have both recently released chips with 80 and 100 cores \cite{tilera, intel}. 
%For example, systems processing real-time business transactions have been implemented by Azul Systems \cite{azul} on top of the highly-parallel Vega3 platform, which consists of up to 864 cores.
%air traffic management using sense and avoid, plug-and-play medical operating rooms,
  Such heterogeneous computing platforms are seeing widespread adoption in many cyber-physical systems (CPS), which are more sophisticated and intelligent computing systems that closely interact with the physical world, including advanced automotive systems, medical CPS, and smart robotics. %Dynamic workloads  seen in such systems, such as recognition, mining and synthesis, present great opportunities for heterogeneous multi-core computing. %To enable intelligent automation of an enormous scale, many CPS have to handle a large variety of resource-demanding workloads. Instead of using multiple networked single-core or multicore chips with small core counts, it would be desirable to use only one or few many-core processors that are highly utilized. Particularly for CPS that often have stringent size, weight, and power (SWaP) constraints, there are strong motivations for utilizing many-core processors. Most importantly, their use can significantly enhance computational performance while dramatically reducing the SWaP consumption. 
 For example, the ARM big.LITTLE, which is a heterogeneous computing architecture integrating relatively slow and fast cores, has been widely adopted in the automotive systems~\cite{?}. This heterogeneous computing technology has been used to serve multiple automotive needs such as infotainment as well as advanced driver assistance systems, and is shown to be able to ``deliver exceptional power and performance that aligns to the vision of scalable solutions for automotive customers~\cite{?}. 
  %?cong: may need to add a more compelling example?
 
 %Tilera's 64-core TILE64 processor has been evaluated~\cite{villalpando2010investigation} for use in the real-time hazard detection and avoidance system of the NASA Altair Lunar Lander~\cite{NASA}. The evaluation showed that ``for computer vision and image analysis the TILE64 architecture provides excellent performance''. This many-core platform has been further recommended for other onboard tasks, including spacecraft and instrument control~\cite{villalpando2010investigation}. 

\vspace{-4mm}
\paragraph{Two Fundamentally Conflicting Goals of Heterogeneous Computing in CPS: Timing Correctness and Energy Guarantees.} 
A major challenge of reliably adopting heterogeneous multicore processors in CPS such as automotive systems is the need to ensure predictable real-time correctness (i.e., enabling timing constraints to be analytically validated at design time).  %which is one of the most important tenets in certification required for such safety-critical systems. 
The functional correctness of a CPS hinges crucially upon the temporal correctness, as the control operations depend on the processing of certain environmental sensing and computation tasks. Establishing real-time correctness requires real-time resource allocation methods, whose focus is on judiciously allocating resources to various tasks so that all the required timing constraints can be satisfied. Although traditional real-time resource allocation methods can ensure timing correctness, they often fail to make or mostly ignore energy guarantees, which is another critical goal of equal importance for many CPS due to the stringent  size, weight, and power (SWaP) constraints imposed by such systems (even with sub-components powered by batteries). Energy guarantees would be critical for safety reasons of many systems and in general beneficial for system designers who have a fixed energy budget and want to support the maximum amount of workloads that require real-time correctness within that budget. 

It is quite challenging to simultaneously consider the two goals of achieving real-time correctness and energy guarantees, because they are fundamentally in conflict. 
On one hand, real-time resource allocation methods often need to maximize the resource utilization and make greedy scheduling choices to guarantee timing correctness for the worst case. On the other hand, however, methods that yield energy guarantees typically require scheduling decisions to be energy-efficient for the current case, implying that in some cases it is wise to scale down the voltage and frequency of certain cores or even force them to idle.(?Cong: Best to incorporate Hank's data support and citations herein.? ) 
 A good real-time resource allocation algorithm may yield excessive amount of energy or thermal hotspots on the many-core chip; while energy-aware methods often fail to make real-time guarantees. %Thus, the problem of many-core resource allocation with the goal of jointly achieving real-time and energy guarantees remains largely open. 
  Besides this challenge, the heterogeneity among cores can greatly complicate resource allocation because ``choices'' must be made when selecting the core upon which a task will execute. When jointly considering timing correctness and energy efficiency in the presence of such heterogeneity and dynamic computing needs, resource allocation becomes even more interesting but complicated due to the huge design space of  timing and energy tradeoffs. Resolving these issues will be the focus of this proposal.

%Besides this challenge, the many-core architecture also brings in new challenges when designing and implementing resource allocation algorithms. Traditional real-time resource allocation methods (e.g., those designed for multiprocessors with shared memory) cannot be applied herein. Major surgeries are needed to develop a comprehensive set of new methods that specifically consider the interesting architectural trade-offs made by \sloppy{commercially}-off-the-shelf many-core processors, such as high-speed on-chip mesh networks, non-uniform shared memory access, and heat sink locations, etc. 
%Furthermore, implementation issues pertaining to using many-core processors for real-time computing have been largely ignored in the literature, where simulation-based experiments are popular choices for evaluation purposes. However, implementation on top of a real many-core processor and evaluation using real-world workloads are needed to convincingly assess the efficacy of any proposed many-core resource allocation and scheduling methods. 
% is in doubt, particularly given the rather complicated processing environment due to the many-core architecture.
%Resolving these issues will be the focus of this proposal.


%Adopting many-core processors brings new challenges due to the complicated many-core architectural design, such as adopting the message passing-based inter-core communication mechanism via network-on-chip.  The workload scheduling and the data routing decisions thus need to be judiciously coordinated to satisfy applications' timing requirements.  

%Such decision making process may get even more complicated due to power and thermal issues (e.g., how to schedule workloads to avoid thermal hot spots), as well as complex application implementations that exhibit highly parallel data dependencies among sub-tasks.


%Traditional real-time resource allocation methods (e.g., those designed for multiprocessors with shared memory) cannot be applied herein. Major surgeries are needed to develop a comprehensive set of new methods that specifically consider the interesting and challenging architectural trade-offs made by \sloppy{commercially}-off-the-shelf many-core processors, such as high-speed on-chip mesh networks, non-uniform shared memory access, specific memory controller and heat sink locations, etc. 

%Furthermore, implementation issues pertaining to using many-core processors for real-time computing have been largely ignored in the literature, where simulation-based experiments are popular choices for evaluation purposes. However, without actual implementation on top of a real many-core processor and evaluation using real-world workloads, the efficacy of any proposed methods is in doubt, particularly given the rather complicated processing environment due to the many-core architecture.

%A major obstacle of widely adopting many-core processors in CPS is the need to ensure predictable temporal correctness (i.e., enabling timing constraints to be analytically validated). Different from many traditional embedded devices whose functionalities are relatively monotonous, the functional correctness of many CPS hinges crucially upon the temporal correctness, as their control operations depend on certain sensing or computation tasks. For instance, a driver-less car needs to recognize a stop sign in a strict real-time manner. The ``stop'' action can happen only after the camera sensor captures the image which is then analyzed by applying certain (computation-intensive) image recognition algorithm. 
%To ensure predictable temporal correctness, real-time scheduling and allocation techniques must be used. While such methods have been well studied on traditional multiprocessor platforms, most existing ones cannot be applied herein because many-core processors (e.g., Tilera TILE64, Intel SCC, Epiphany-IV) make several new and interesting architectural trade-offs such as high-speed on-chip mesh networks, non-uniform shared memory access, lower clock frequencies, specific heat sink location, etc.. Predictable and verifiable real-time resource allocation issues that arise pertaining to many-core processors have received relatively little attention. Such issues are the focus of this proposal. 

\vspace{-4mm}
\paragraph{Proposed approach.} 
Most existing works on real-time resource allocation in heterogeneous computing systems focus on guaranteeing timing correctness (e.g., see \cite{raravi2014task, raravi2013assigning, niemeier2011partitioned} for an overview). %Existing works on energy-efficient real-time resource allocation for homogeneous systems~\cite{?} cannot be efficiently applied herein because 
 A few recent works~\cite{?} focusing on exploring the timing/energy tradeoff space in a heterogeneous computing system made the following critical assumption: all processor cores can operate at max speed all the time while sustaining safely. %, and (\textit{ii}) heterogeneous processor cores exhibit similar variances in power and performance characteristics with respect to dynamic voltage and frequency scaling (DVFS). 
  This assumption unfortunately invalid due to the critical ``dark silicon'' problem, i.e., essentially all processors are over-provisioned and have much larger max compute capacity than they can safely sustain (this has never been true for embedded systems before but it is now~\cite{?}). The thrust of this research is to simultaneously achieve the goals of timing correctness and energy guarantees on heterogeneous computing platforms containing processor cores with varying speeds by answering the following research questions: \textbf{(\textit{i}) how to guarantee timing while running processor cores as slow as possible to respect given energy budgets?}   \textbf{(\textit{ii}) how to quickly detect and avoid timing errors by utilizing the over-provisioned processing capacity in short bursts?} Our proposed research is fundamentally motivated by the observations that (\textit{i}) it is significantly more energy efficient when slowing down processor cores (compared to the race-to-idle strategy), and (\textit{ii}) processor cores cannot operate at max speed all the time and are (sometimes significantly) over-provisioned for what they can sustain safely. (Sec.~\ref{sec:motivation} will provide detailed data support for these observations.) 
  
  ?Cong: the above paragraph needs more work, as I feel I have not concisely and clearly shown the unique angle that was considered in existing works. That invalid assumption made in existing works needs to be re-worked.?

 %In most prior work on real-time resource allocation for multiprocessor platforms that assume shared memory, the scheduling issues have received much attention (i.e., which task should be scheduled when); while the mapping issue (i.e., which physical core should be selected for executing the scheduled tasks) and the data communication issues (i.e., inter-core communications) have been largely ignored. On a traditional shared-memory multiprocessor platform, such ignorance is reasonable since all cores are homogeneous and communicate via the same memory space. However, for a many-core processor that implements inter-core communication mainly via network-on-chi, the task-to-core mapping and data communication issues become significant because they considerably impact  many performance aspects including response times, routing latency, communication congestion, heat dissipation, and cache coherence. Our research thesis is thus to view scheduling, mapping, and communication as an integrated set of ``first-class citizens'' in the design for real-time many-core computing.


%Most prior work on real-time resource allocation for many-core computing systems (e.g., see \cite{?} for an overview) takes an ``atomistic'' approach by considering issues involved in many-core computing separately, including guaranteeing real-time correctness, reducing power and energy consumption, and minimizing routing latency and overheads. %, and maintaining thermal temperatures under certain thresholds. 

%However, we argue that a ``holistic'' approach must be employed that consider all such factors when making workload mapping, resource scheduling, data routing decisions in order to realize efficient real-time many-core computing in practice, because there often exist strong correlations among many of these decisions. Moreover, most existing real-time resource allocation methods designed for many-core computing platforms are either heuristic-based (i.e., no real-time correctness guarantee can be analytically validated), or assume rather simple workload models that are insufficient to represent real-world applications. Furthermore, power- and thermal-related issues under the real-time computing context have either been largely ignored or simply treated as ``best-effort'' optimization objectives in existing research. We believe that such issues must be regarded as ``first-class citizens'' when designing real-time resource allocation methods, given their importance to guarantee safe operations in many mission-critical CPS.

%Motivated by these observations, the goal of this research is to achieve predictable real-time computing on many-core processors with network-on-chip by \textbf{(\textit{i}) establishing fundamental real-time resource allocation methods that can analytically guarantee real-time correctness for rather complex workloads (e.g., those implemented using general graph-based formalism)}, \textbf{(\textit{ii)} developing a holistic approach that coordinates workload mapping, resource scheduling, and data routing that can guarantee real-time correctness while optimizing other key performance metrics including communication delay and maximum temperature},    %minimizing power consumption and/or avoiding thermal hotspots on the many-core chip},  
%\textbf{(\textit{iii}) given specific power budget and thermal thresholds, deriving resource allocation and data routing protocols that can support the maximum set of workloads with guaranteed real-time correctness},
%and \textbf{(\textit{iv}) conducting a real implementation of the proposed real-time resource allocation methods on a Tilera many-core processor and evaluating the efficacy via large-scale experiments and case studies}.

We intend to show that the fundamental timing/energy tradeoffs can be explored by leveraging recent research by the PI's group that has led to a new set of \textit{optimal} resource allocation algorithms for uniform heterogeneous multicore-based systems containing processor cores with varying speeds~\cite{Zhou2014a, Liu1, Liu2, Liu6, Liu7}. These algorithms can analytically guarantee fast and analytically bounded response times for complex workloads implemented using rather general graph-based formalism and executed in a heterogeneous computing system.  %while minimizing the communication latency among tasks with data dependency if allocated to different nodes in the system. 
We will consider such algorithms as the basis for determining the best configurations of resource allocation  strategies and processor cores' dynamic voltage and frequency scaling (DVFS) settings, which are most energy efficient for a given set of workloads. Significant further development is needed that considers the dynamic and heterogeneous performance and energy characteristics exhibited on the processor cores.
 Since there are numerous choices that can be made regarding resource allocation algorithm,  the task-to-core mapping strategy, and DVFS settings, the potential solution space for this problem is vast. Tasks can be allocated for CPU resources globally (i.e., a task can be mapped onto any resource) or via partitioning (a task can only be mapped onto a designated resource). Also, task priorities may be either static or dynamic. %Moreover, mapping schemes may differ according to the metrics we select to optimize (it is less likely to design an ``one for all'' scheme for different considered metrics). 
 The efficiency of DVFS-incurred energy saving can also be different for cores with different speeds. 
 In this project, we propose to carefully evaluate the various alternatives in the space of potential solutions on top of real hardware and determine which configurations are preferable are given workloads. An real implementation-based empirical evaluation is thus a focus of this project.


 %The goal is to consider the specific many-core architecture features into the scheduler design and eliminate undesirable properties such as communication congestion, routing delay, and thermal hotspots.  %\textit{The problem to be addressed in this research is to determine which combination of real-time scheduling algorithm with specific mapping and data routing protocol imposes the least constraints on considered metrics including schedulability, communication latency, power consumption, and temperature.} The term \textit{schedulability} refers to the ability to analytically validate the required timing constraints of all workloads.

%Given such a large solution space, using real implementation-based empirical evaluations will provide a good assessment approach on the success of the proposed research.

\vspace{-2mm}
\paragraph{Research objectives.} We will pursue the following four-step plan to accomplish our research goal.
\begin{itemize}
\item \textbf{Step1: Identify the most energy-efficient configurations of resource allocation and DVFS for a given workload that guarantee timing:} We will first design workload mapping and resource scheduling algorithms for processing workloads on a heterogeneous multicore processor. The goal is to  guarantee timing while minimizing energy consumption by running processor cores as slow as possible. For each devised algorithm, formal timing validation tests will be developed.  
\item \textbf{Step 2: Detect and avoid timing errors.} Although slowing down cores may be most energy-efficient, it may cause timing errors more easily (e.g., due to sudden workload bursts). Thus, we will further develop robust mechanisms for quickly detecting and avoiding timing errors. Our main idea is to achieve this goal through exploiting the over-provisioned processing capacity in short bursts.
\item \textbf{Step 3: Carry out overhead-aware schedulability studies.} The research in the first two steps will provide solutions for simultaneously achieving timing correctness and energy guarantees on heterogeneous computing platforms. To evaluate their effectiveness in practice, we will incorporate them in real hardware (using the ARM big.LITTLE architecture) and conduct large-scale overhead-aware schedulability experiments with both synthetic workloads with widely varied parameters and benchmarks. 
 We plan to apply an extensive methodology that is designed for comparing real-time resource allocation strategies in an overhead-cognizant way \cite{clustered, bastoni2010, BBBdissertation}, which is proven to be effective for many real-time application systems. 
\item \textbf{Step 4: Conduct case-studies.} To determine if our proposed methods can be applied in practice, we intend to conduct  case-study evaluations using several specific real-time workloads seen in automotive systems. For example, one such application we will consider is real-time object recognition, which is heavily performed in automotive systems for implementing tools such as collision avoidance and traffic sign recognition. We will evaluate the performance in terms of several specific metrics for using one or more preferable configurations identified in Step~3 to support such workloads. 
\end{itemize}

\paragraph{Qualification.} The PIs are well-qualified to conduct this research. The proposing team has worked on various aspects of real-time systems, heterogeneous multicore computing, and energy optimization in the past years, ranging from theoretical real-time analysis~\cite{?}, real-time operating systems~\cite{?}, heterogeneous multicore architecture~\cite{?}, and energy efficient computing under various computer architectures~\cite{?}. These efforts have resulted in a number of publications accepted by several systems- and architecture-related premium conference and journal venues such as SOSP, RTSS, ISCA, ASPLOS, IPDPS, Micro, PACT, and IEEE Transactions on Parallel and Distributed Systems. 
PI Liu has been working on developing device driver and OS support for heterogeneous real-time systems accelerated by co-processors such as GPUs~\cite{Liu12, liu2009general, liu2008distributed, liu2008pass, liu2007general, liu2008scheduling, liu2008heavyweight, liu2006distributed, liu2009scalable},  resource allocation on heterogeneous platforms~\cite{?}, and real-time schedulability analysis~\cite{Liu1, Liu2, Liu6, Liu7, Liu10, liu2014supporting, elliott1minimizing, hereconfiguration, he2013exploring, Liu3, Liu4, Liu5, Liu9, Liu11, Liu13}.
PI Hoffman at the University of Chicago has rich experience in ?INPUT FROM HANK?